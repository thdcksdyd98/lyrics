{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dated-ambassador",
   "metadata": {},
   "source": [
    "# 목차\n",
    "\n",
    "##### 1. 노드 실습\n",
    "##### 2. 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-hampton",
   "metadata": {},
   "source": [
    "# 노드 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appointed-hospital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 파일을 읽기모드로 열고\n",
    "# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:\n",
    "    raw_corpus = f.read().splitlines()\n",
    "\n",
    "# 앞에서부터 10라인만 화면에 출력해 볼까요?\n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "private-sheriff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "# 1차 필터링. 화자가 표시된 문장과 공백인 문장을 처리해줌\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "virgin-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 (띄어쓰기를 기준으로)\n",
    "# 3가지 문제 케이스\n",
    "# 1. 문장 부호 2. 대소문자 3. 특수문자\n",
    "\n",
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "choice-cartoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excess-breath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f2169686090>\n"
     ]
    }
   ],
   "source": [
    "# 우리가 처음 언어를 배울때 모국어로 된 표현으로 공부를 하듯, 컴퓨터도 숫자로 배우게 된다.\n",
    "# 이 과정에서 텐서플로우를 사용. 이 패키지는 정제된 데이터를 토큰화 하고 단어사전을 만들어 주며, 데이터를\n",
    "# 숫자로 변환까지 해줌. 이 과정을 벡터화 라고 하고 숫자로 변환된 데이터를 텐서라고 한다.\n",
    "\n",
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inner-directive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "# 텐서 데이터는 모두 정수로 이루어져 있습니다. 이 숫자는 다름 아니라, tokenizer에 구축된 단어 사전의 인덱스입니다.\n",
    "# 단어 사전이 어떻게 구축되었는지 아래와 같이 확인해 봅시다.\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "external-senior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워 넣은 것입니다. \n",
    "# 사전에는 없지만 0은 바로 패딩 문자 <pad>가 될 것입니다.\n",
    "\n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "superb-sharp",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus 내의 첫 번째 문장에 대해 생성된 소스와 타겟 문장을 확인해 보았습니다. \n",
    "# 예상대로 소스는 2(<start>)에서 시작해서 3(<end>)으로 끝난 후 0(<pad>)로 채워져 있습니다. \n",
    "# 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한 칸 시프트 한 형태를 가지고 있습니다\n",
    "\n",
    "# 마지막으로 우리는 데이터셋 객체를 생성할 것입니다. \n",
    "# 그동안 우리는 model.fit(x_train, y_train, ...) 형태로 Numpy Array 데이터셋을 생성하여 model에 제공하는 형태의 학습을 많이 진행해 왔습니다. \n",
    "# 그러나 텐서플로우를 활용할 경우 텐서로 생성된 데이터를 이용해 tf.data.Dataset객체를 생성하는 방법을 흔히 사용합니다. \n",
    "# tf.data.Dataset객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의 기능을 제공하므로 꼭 사용법을 알아 두시기를 권합니다. \n",
    "# 우리는 이미 데이터셋을 텐서 형태로 생성해 두었으므로, tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성할 것입니다.\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thorough-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "optimum-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-advice",
   "metadata": {},
   "source": [
    "Embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 줍니다. 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현(representation)으로 사용됩니다.\n",
    "\n",
    "위 코드에서 embedding_size 는 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기입니다. 만약 그 크기가 2라면 예를 들어\n",
    "\n",
    "- 차갑다: [0.0, 1.0]\n",
    "- 뜨겁다: [1.0, 0.0]\n",
    "- 미지근하다: [0.5, 0.5]\n",
    "\n",
    "값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 오히려 혼란만을 야기할 수 있습니다. 이번 실습에서는 256이 적당해 보이네요.\n",
    "\n",
    "STM 레이어의 hidden state 의 차원수인 hidden_size 도 같은 맥락입니다. hidden_size 는 모델에 얼마나 많은 일꾼을 둘 것인가? 로 이해해도 크게 엇나가지 않습니다. 그 일꾼들은 모두 같은 데이터를 보고 각자의 생각을 가지는데, 역시 충분한 데이터가 주어지면 올바른 결정을 내리겠지만 그렇지 않으면 배가 산으로 갈 뿐 입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "competent-heart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-4.24588827e-04,  2.55890220e-04,  1.57685732e-04, ...,\n",
       "          3.98655538e-04,  1.09762273e-04,  1.10298795e-04],\n",
       "        [-4.29135747e-04,  4.31284046e-04,  1.42029283e-04, ...,\n",
       "          7.29580468e-04,  3.09079682e-04,  2.49494420e-04],\n",
       "        [-4.11398709e-04,  3.26509587e-04,  6.33867894e-05, ...,\n",
       "          8.25205410e-04,  4.08633263e-04,  2.50226527e-04],\n",
       "        ...,\n",
       "        [ 1.12994062e-03,  1.02787639e-03,  7.53116386e-04, ...,\n",
       "         -1.78548321e-03, -1.51731481e-03,  4.61191608e-04],\n",
       "        [ 1.20162778e-03,  1.46226631e-03,  1.01114088e-03, ...,\n",
       "         -1.93355081e-03, -1.80225563e-03,  2.07585745e-05],\n",
       "        [ 1.25800772e-03,  1.89239380e-03,  1.23049412e-03, ...,\n",
       "         -2.07765540e-03, -2.08243821e-03, -4.02214617e-04]],\n",
       "\n",
       "       [[-4.24588827e-04,  2.55890220e-04,  1.57685732e-04, ...,\n",
       "          3.98655538e-04,  1.09762273e-04,  1.10298795e-04],\n",
       "        [-2.19430003e-04,  6.88256056e-04,  7.71835403e-05, ...,\n",
       "          4.36709088e-04,  6.43923995e-05,  2.52574566e-04],\n",
       "        [ 1.45289769e-05,  6.32498937e-04, -2.51877180e-04, ...,\n",
       "          4.46635211e-04, -1.08419845e-04,  6.27912930e-04],\n",
       "        ...,\n",
       "        [ 1.09748787e-03, -1.13691785e-03,  5.42197027e-04, ...,\n",
       "         -1.73405162e-03,  1.10780343e-03,  2.55562132e-04],\n",
       "        [ 1.04172493e-03, -6.50605478e-04,  7.06319639e-04, ...,\n",
       "         -1.78349495e-03,  6.64051156e-04,  7.93233412e-05],\n",
       "        [ 1.02771271e-03, -1.14386734e-04,  8.67424766e-04, ...,\n",
       "         -1.83480594e-03,  1.90488747e-04, -1.57376722e-04]],\n",
       "\n",
       "       [[-4.24588827e-04,  2.55890220e-04,  1.57685732e-04, ...,\n",
       "          3.98655538e-04,  1.09762273e-04,  1.10298795e-04],\n",
       "        [-7.89782556e-04,  5.55475184e-04,  3.69735790e-04, ...,\n",
       "          1.92548498e-04,  1.50517968e-04,  3.07161645e-05],\n",
       "        [-5.78401785e-04,  5.95389167e-04,  2.19383262e-04, ...,\n",
       "          2.09675331e-04,  2.39550427e-04,  8.87512724e-05],\n",
       "        ...,\n",
       "        [ 1.32570567e-04,  1.44958915e-03,  1.56608759e-03, ...,\n",
       "         -1.62597210e-03, -1.44848798e-03, -3.64323176e-04],\n",
       "        [ 2.62688613e-04,  1.79911149e-03,  1.68770924e-03, ...,\n",
       "         -1.80009042e-03, -1.70432136e-03, -6.28693553e-04],\n",
       "        [ 3.94908391e-04,  2.16511986e-03,  1.79047103e-03, ...,\n",
       "         -1.96718704e-03, -1.95976649e-03, -9.03367589e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-4.24588827e-04,  2.55890220e-04,  1.57685732e-04, ...,\n",
       "          3.98655538e-04,  1.09762273e-04,  1.10298795e-04],\n",
       "        [-4.76016576e-04,  3.17641097e-04, -9.00775995e-05, ...,\n",
       "          6.96870440e-04, -1.57334667e-04,  2.86084716e-04],\n",
       "        [-5.17472683e-04,  2.62221060e-04, -5.21148431e-05, ...,\n",
       "          3.87920300e-04, -3.41522798e-04,  4.54713212e-04],\n",
       "        ...,\n",
       "        [ 1.38651463e-04,  8.23281123e-04,  1.06436468e-03, ...,\n",
       "         -2.83965864e-03, -2.12335563e-03, -1.05966418e-03],\n",
       "        [ 2.80932785e-04,  1.21087581e-03,  1.17925834e-03, ...,\n",
       "         -2.81864963e-03, -2.28532078e-03, -1.28415262e-03],\n",
       "        [ 4.18644195e-04,  1.62033376e-03,  1.28815265e-03, ...,\n",
       "         -2.81937909e-03, -2.44915858e-03, -1.51010510e-03]],\n",
       "\n",
       "       [[-4.24588827e-04,  2.55890220e-04,  1.57685732e-04, ...,\n",
       "          3.98655538e-04,  1.09762273e-04,  1.10298795e-04],\n",
       "        [-3.09923984e-04,  5.09862439e-04,  2.94400124e-05, ...,\n",
       "          4.92752588e-04,  4.55620553e-04,  3.12352757e-04],\n",
       "        [-1.88715785e-04,  6.38910045e-04,  1.71533567e-04, ...,\n",
       "          1.08559972e-04,  5.28283883e-04, -8.57884952e-05],\n",
       "        ...,\n",
       "        [ 4.95516229e-04,  1.75677252e-03,  1.33886072e-03, ...,\n",
       "         -2.11982219e-03, -7.69437407e-04, -6.47650100e-04],\n",
       "        [ 6.74131676e-04,  2.16536759e-03,  1.47306488e-03, ...,\n",
       "         -2.28167907e-03, -1.16504531e-03, -9.70850349e-04],\n",
       "        [ 8.25499883e-04,  2.55743950e-03,  1.59254158e-03, ...,\n",
       "         -2.42859428e-03, -1.53795164e-03, -1.26391090e-03]],\n",
       "\n",
       "       [[-4.24588827e-04,  2.55890220e-04,  1.57685732e-04, ...,\n",
       "          3.98655538e-04,  1.09762273e-04,  1.10298795e-04],\n",
       "        [-3.60253471e-04,  6.51817536e-04,  2.37796557e-04, ...,\n",
       "          7.82621675e-04,  7.94091829e-05,  2.27345765e-04],\n",
       "        [-2.19949638e-04,  5.44734765e-04,  1.83805081e-04, ...,\n",
       "          8.08951736e-04,  1.03904218e-04,  4.31312103e-04],\n",
       "        ...,\n",
       "        [ 1.12165511e-03,  3.04745976e-03,  2.73250649e-03, ...,\n",
       "         -2.67223129e-03, -2.56710406e-03, -1.50058069e-03],\n",
       "        [ 1.11929246e-03,  3.39300511e-03,  2.70407042e-03, ...,\n",
       "         -2.78657116e-03, -2.79837358e-03, -1.71410700e-03],\n",
       "        [ 1.11143256e-03,  3.70936096e-03,  2.67665903e-03, ...,\n",
       "         -2.88256234e-03, -3.00784269e-03, -1.89253700e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아직 모델을 빌드 하지 않았다. 이럴때 모델에 데이터를 조금 넣어보면 좋다.\n",
    "\n",
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-approach",
   "metadata": {},
   "source": [
    "모델의 최종 출력 텐서 shape를 유심히 보면 shape=(256, 20, 7001)임을 알 수 있습니다. 7001은 Dense 레이어의 출력 차원수입니다. 7001개의 단어 중 어느 단어의 확률이 가장 높을지를 모델링해야 하기 때문입니다.\n",
    "256은 이전 스텝에서 지정한 배치 사이즈입니다. dataset.take(1)를 통해서 1개의 배치, 즉 256개의 문장 데이터를 가져온 것입니다.\n",
    "\n",
    "그렇다면 20은 무엇을 의미할까요? 비밀은 바로 tf.keras.layers.LSTM(hidden_size, return_sequences=True)로 호출한 LSTM 레이어에서 return_sequences=True이라고 지정한 부분에 있습니다. 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다. 만약 return_sequences=False였다면 LSTM 레이어는 1개의 벡터만 출력했을 것입니다.\n",
    "\n",
    "그런데 문제는, 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모른다는 점입니다. 모델을 만들면서 알려준 적도 없습니다. 그럼 20은 언제 알게된 것일까요? 네, 그렇습니다. 데이터를 입력받으면서 비로소 알게 된 것입니다. 우리 데이터셋의 max_len이 20으로 맞춰져 있었던 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "electronic-smell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 39s 368ms/step - loss: 4.3828\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 34s 370ms/step - loss: 2.8137\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 35s 373ms/step - loss: 2.7145\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 35s 375ms/step - loss: 2.6052\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 35s 379ms/step - loss: 2.5316\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 35s 380ms/step - loss: 2.4877\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 2.4250\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 36s 382ms/step - loss: 2.3780\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 2.3096\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 2.2609\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 36s 383ms/step - loss: 2.2083\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 36s 382ms/step - loss: 2.1567\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 36s 382ms/step - loss: 2.1056\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 2.0572\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.9976\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.9516\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.8994\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.8443\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 35s 380ms/step - loss: 1.7994\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.7476\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.6950\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 35s 380ms/step - loss: 1.6405\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 35s 382ms/step - loss: 1.5865\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 36s 383ms/step - loss: 1.5376\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.4843\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 36s 383ms/step - loss: 1.4268\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 36s 382ms/step - loss: 1.3811\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.3260\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.2691\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 35s 381ms/step - loss: 1.2141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2168c10f90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이제 모델이 학습할 준비가 완료되었습니다. 아래 코드를 실행해 모델을 학습시켜보세요!\n",
    "\n",
    "# optimizer와 loss등은 차차 배웁니다\n",
    "# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "addressed-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-escape",
   "metadata": {},
   "source": [
    "학습 단계에서 우리는 이런 while 문이 필요 없었습니다. 소스 문장과 타겟 문장이 있었고, 우리는 소스 문장을 모델에 입력해서 나온 결과를 타겟 문장과 직접 비교하면 그만이었습니다.\n",
    "그러나 텍스트를 실제로 생성해야 하는 시점에서, 우리에게는 2가지가 없습니다. 하나는 타겟 문장입니다. 또 하나는 무엇이냐 하면, 소스 문장입니다. 생각해 보면 우리는 텍스트 생성 태스크를 위해 테스트 데이터셋을 따로 생성한 적이 없습니다.\n",
    "\n",
    "generate_text() 함수에서 init_sentence를 인자로 받고는 있습니다. 이렇게 받은 인자를 일단 텐서로 만들고 있습니다. 디폴트로는 `<start>` 단어 하나만 받는군요.\n",
    "    \n",
    "    \n",
    "while의 첫 번째 루프에서 test_tensor에 `<start>` 하나만 들어갔다고 합시다. 우리의 모델이 출력으로 7001개의 단어 중 A를 골랐다고 합시다.\n",
    "while의 두 번째 루프에서 test_tensor에는 `<start>` A가 들어갑니다. 그래서 우리의 모델이 그다음 B를 골랐다고 합시다.\n",
    "while의 세 번째 루프에서 test_tensor에는 `<start>` A B가 들어갑니다. 그래서..... (이하 후략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "creative-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s not prepared for death . <end> '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-humidity",
   "metadata": {},
   "source": [
    "# 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "early-surprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽어오기\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "responsible-aquatic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They come from everywhere\n",
      "A longing to be free\n",
      "They come to join us here\n",
      "From sea to shining sea And they all have a dream\n",
      "As people always will\n",
      "To be safe and warm\n",
      "In that shining city on the hill Some wanna slam the door\n",
      "Instead of opening the gate\n",
      "Aw, let's turn this thing around\n",
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 데이터 정제\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue \n",
    "    if sentence[-1] == \"]\": continue  # [verse] 부분 삭제\n",
    "    if sentence[-1] == \":\": continue\n",
    "\n",
    "    if idx > 9: break\n",
    "        \n",
    "    print(sentence)\n",
    "    \n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>' \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "under-input",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> they come from everywhere <end>',\n",
       " '<start> a longing to be free <end>',\n",
       " '<start> they come to join us here <end>',\n",
       " '<start> from sea to shining sea and they all have a dream <end>',\n",
       " '<start> as people always will <end>',\n",
       " '<start> to be safe and warm <end>',\n",
       " '<start> in that shining city on the hill some wanna slam the door <end>',\n",
       " '<start> instead of opening the gate <end>',\n",
       " '<start> aw , let s turn this thing around <end>',\n",
       " '<start> it s up to me and you <end>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \"]\": continue\n",
    "\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "duplicate-genesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   45   66 ...    0    0    0]\n",
      " [   2    9 3375 ...    0    0    0]\n",
      " [   2   45   66 ...    0    0    0]\n",
      " ...\n",
      " [   2  563   21 ...    0    0    0]\n",
      " [   2  120   34 ...    0    0    0]\n",
      " [   2    5   22 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f204cac6650>\n"
     ]
    }
   ],
   "source": [
    "# 벡터화\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    # 토큰의 개수를 15개로 지정\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "marked-tutorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "coupled-direction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  45  66  74 799   3   0   0   0   0   0   0   0   0]\n",
      "[ 45  66  74 799   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wooden-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터셋 분리\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "norwegian-union",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (139790, 14)\n",
      "Target Train: (139790, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "agricultural-sugar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인공지능 만들기\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "owned-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "medieval-second",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 2.0140727e-04,  2.0616390e-04, -4.2554113e-04, ...,\n",
       "         -1.5800384e-04,  2.1031841e-04, -2.3018006e-04],\n",
       "        [ 2.1496012e-04,  3.2461269e-04, -6.9152931e-04, ...,\n",
       "         -1.4715282e-04,  2.6054820e-04,  8.2834158e-05],\n",
       "        [ 3.6073948e-04,  8.8138768e-04, -9.1926730e-04, ...,\n",
       "         -5.7281206e-05,  5.6868704e-04, -4.9744444e-06],\n",
       "        ...,\n",
       "        [ 9.3532767e-04,  3.3292603e-03, -1.3484472e-03, ...,\n",
       "         -6.7793665e-04,  7.4223639e-04, -3.8729122e-04],\n",
       "        [ 7.2741375e-04,  3.0121945e-03, -1.0211664e-03, ...,\n",
       "         -4.7501744e-04,  7.0931012e-04, -4.0866400e-04],\n",
       "        [ 2.6291996e-04,  2.5508192e-03, -7.5002154e-04, ...,\n",
       "         -5.6640233e-04,  1.0886755e-03, -3.9751059e-04]],\n",
       "\n",
       "       [[ 3.4233587e-04, -1.3250958e-04,  1.7199527e-04, ...,\n",
       "          9.1632945e-05,  1.5460582e-04, -1.9369171e-04],\n",
       "        [ 5.2243553e-04, -5.4761732e-04,  2.3239070e-04, ...,\n",
       "          1.7637172e-04,  6.5239372e-05, -1.3927015e-04],\n",
       "        [ 2.7964317e-04, -1.5043103e-03,  3.7820384e-04, ...,\n",
       "          1.0916436e-04,  3.7741211e-05, -8.1919643e-05],\n",
       "        ...,\n",
       "        [ 3.1300363e-04, -1.1159006e-03,  2.1318239e-03, ...,\n",
       "          1.4411277e-03,  9.5737814e-05, -2.4308877e-03],\n",
       "        [ 5.2955333e-04, -9.7377400e-04,  2.5760911e-03, ...,\n",
       "          1.6752821e-03,  2.2347456e-04, -2.9045567e-03],\n",
       "        [ 7.7974726e-04, -8.0317358e-04,  2.9497272e-03, ...,\n",
       "          1.9002437e-03,  3.6222546e-04, -3.2850439e-03]],\n",
       "\n",
       "       [[ 3.4233587e-04, -1.3250958e-04,  1.7199527e-04, ...,\n",
       "          9.1632945e-05,  1.5460582e-04, -1.9369171e-04],\n",
       "        [ 4.1455700e-04, -4.2416505e-05,  4.1533119e-04, ...,\n",
       "         -2.3508430e-04,  1.3423785e-04, -1.9714549e-04],\n",
       "        [ 5.2096276e-04, -1.9839418e-04,  5.3374120e-04, ...,\n",
       "         -5.5179687e-04, -3.1230430e-04, -6.9743706e-05],\n",
       "        ...,\n",
       "        [-3.5603496e-04,  4.5717761e-04,  3.4198930e-04, ...,\n",
       "         -1.6434038e-03, -1.4255911e-03,  4.2133458e-04],\n",
       "        [-7.6092416e-05,  3.1589123e-04,  7.5315667e-04, ...,\n",
       "         -1.6130705e-03, -1.4884372e-03, -7.0167938e-05],\n",
       "        [ 1.4656756e-04,  1.0625258e-04,  1.2952071e-03, ...,\n",
       "         -1.3803018e-03, -1.4351782e-03, -7.0006214e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.4233587e-04, -1.3250958e-04,  1.7199527e-04, ...,\n",
       "          9.1632945e-05,  1.5460582e-04, -1.9369171e-04],\n",
       "        [ 4.9220701e-04, -1.8681581e-04,  2.8431328e-04, ...,\n",
       "          1.9633680e-04,  1.2859517e-04, -4.0804580e-04],\n",
       "        [ 8.2107494e-04, -1.3072933e-04,  1.8584005e-05, ...,\n",
       "          2.4426612e-04,  2.0379663e-04, -5.2112638e-04],\n",
       "        ...,\n",
       "        [ 4.8359486e-04,  2.9187134e-04,  1.5118876e-03, ...,\n",
       "          5.7680800e-04, -4.1040237e-04, -1.6249251e-03],\n",
       "        [ 5.4901768e-04,  1.9455615e-04,  2.0459376e-03, ...,\n",
       "          7.9725689e-04, -3.3169886e-04, -2.2348370e-03],\n",
       "        [ 6.9504732e-04,  1.4339043e-04,  2.5194904e-03, ...,\n",
       "          1.0418372e-03, -2.1171861e-04, -2.7640937e-03]],\n",
       "\n",
       "       [[ 3.4233587e-04, -1.3250958e-04,  1.7199527e-04, ...,\n",
       "          9.1632945e-05,  1.5460582e-04, -1.9369171e-04],\n",
       "        [ 7.1918441e-04, -1.8162660e-05, -1.4166891e-05, ...,\n",
       "          9.0182541e-05,  1.9822284e-04, -3.9439339e-05],\n",
       "        [ 7.5148360e-04, -1.9422344e-04, -2.4251582e-04, ...,\n",
       "         -1.2638095e-04, -6.3116022e-06, -1.1020418e-04],\n",
       "        ...,\n",
       "        [-1.5172187e-03, -6.6214602e-04,  1.9596610e-03, ...,\n",
       "         -4.0315028e-04, -2.2522044e-04, -3.1964498e-04],\n",
       "        [-1.0560497e-03, -7.6140580e-04,  2.4151437e-03, ...,\n",
       "         -1.4612873e-04, -2.5496009e-04, -1.0258814e-03],\n",
       "        [-5.6822202e-04, -8.5314276e-04,  2.8498662e-03, ...,\n",
       "          1.7215789e-04, -2.2509386e-04, -1.7207309e-03]],\n",
       "\n",
       "       [[ 3.4233587e-04, -1.3250958e-04,  1.7199527e-04, ...,\n",
       "          9.1632945e-05,  1.5460582e-04, -1.9369171e-04],\n",
       "        [ 6.1381009e-04, -1.9471672e-04,  4.6489795e-04, ...,\n",
       "          8.8967026e-06,  3.2319769e-04, -7.2805560e-04],\n",
       "        [ 8.4340252e-04, -6.8926287e-04,  2.6675209e-04, ...,\n",
       "          1.9043511e-04,  2.6546337e-04, -7.9729647e-04],\n",
       "        ...,\n",
       "        [-1.3960354e-03,  3.9348623e-04,  4.5428055e-04, ...,\n",
       "         -2.0725049e-05, -8.7090826e-04, -8.2706136e-04],\n",
       "        [-1.1791689e-03,  3.8718735e-04,  8.3307113e-04, ...,\n",
       "          6.7855181e-06, -8.7237667e-04, -1.2389136e-03],\n",
       "        [-9.2760660e-04,  2.6241044e-04,  1.3117373e-03, ...,\n",
       "          1.2251422e-04, -8.3315186e-04, -1.7578482e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dressed-blake",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  20979712  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 85,276,897\n",
      "Trainable params: 85,276,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "affecting-event",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "682/682 [==============================] - 714s 1s/step - loss: 3.9042 - val_loss: 2.8849\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 713s 1s/step - loss: 2.8163 - val_loss: 2.4102\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 713s 1s/step - loss: 2.3799 - val_loss: 2.0081\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 712s 1s/step - loss: 1.9943 - val_loss: 1.6638\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 713s 1s/step - loss: 1.6663 - val_loss: 1.3935\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 711s 1s/step - loss: 1.3999 - val_loss: 1.1940\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 710s 1s/step - loss: 1.2050 - val_loss: 1.0525\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 710s 1s/step - loss: 1.0713 - val_loss: 0.9683\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 711s 1s/step - loss: 0.9888 - val_loss: 0.9208\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 710s 1s/step - loss: 0.9461 - val_loss: 0.8957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20352b2390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# validation loss\n",
    "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ideal-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        predict = model(test_tensor) \n",
    "\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "occupational-salad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love the way you shake that thing girl <end> '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-friend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "damaged-somewhere",
   "metadata": {},
   "source": [
    "# conclusion\n",
    "\n",
    "다행히 모델이 정상적으로 작동하고 결과물도 내 기준에서는 사람이 만든 가사처럼 느껴졌다. 물론 모든 코드를 다 이해한것도 아니고 설령 그런 부분이 있었더라도 다른 프로젝트에서 어떻게 이용할지는 아직 감이 잡히지 않는다. 그건 미래의 내가 하겠지.... 이번 노드를 할 때 조금 힘든 부분은 loss를 줄이는 부분이었다. 다만, 이 부분은 embedding size와 hidden size를 두배로 지정하니 loss값이 줄어들었다. 확실한건 아니지만, 데이터를 더 많이 넣었을때 정확도가 조금 올라가는 원리와 비슷한게 아닐까 싶다. 이번 노드를 하면서 느낀 점은 아직도 내가 너무 부족하다는 것이다. 늘 하는 이야기지만 열심히 하는 수 밖에는 없을거 같다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-blogger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
